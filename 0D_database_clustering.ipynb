{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0D database clustering\n",
    "\n",
    "In this notebook we load a database (generated with file *0D_database_generation.ipynb*) and apply K-means clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab preparation\n",
    "\n",
    "These lines are here to enable Colab running of the tools. We need to perform a git clone in order to have access to python scripts. This needs to be done at each runtime as the clone is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if use_colab:\n",
    "    !git clone -b cost_course_exercices https://github.com/cmehl/COST_lecture.git\n",
    "    !pip install PyDOE\n",
    "    !pip install cantera\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Create a folder in the root directory\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/COST_lecture_data\"):\n",
    "        !mkdir -p \"/content/drive/MyDrive/COST_lecture_data\"\n",
    "    else:\n",
    "        print(\"Folder /content/drive/MyDrive/COST_lecture_data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if use_colab:\n",
    "    from COST_lecture.chem_ai.utils import StandardScaler\n",
    "else:\n",
    "    from chem_ai.utils import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the database. We define the folder to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    folder = \"/content/drive/MyDrive/ML_chem_data/case_0D_highT\"\n",
    "else:\n",
    "    folder = \"./case_0D_test\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract associated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder, \"dtb_params.json\"), \"r\") as file:\n",
    "    dtb_params = json.load(file)\n",
    "\n",
    "fuel = dtb_params[\"fuel\"]\n",
    "mech_file = dtb_params[\"mech_file\"]\n",
    "log_transform = dtb_params[\"log_transform\"]\n",
    "threshold = dtb_params[\"threshold\"]\n",
    "p = dtb_params[\"p\"]\n",
    "dt = dtb_params[\"dt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data. It is here the raw unscaled data. We load both the training and validation data, but only the training data will be used to define the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join(folder,\"X_train_raw.csv\"))\n",
    "Y_train = pd.read_csv(os.path.join(folder,\"Y_train_raw.csv\"))\n",
    "\n",
    "X_val = pd.read_csv(os.path.join(folder,\"X_val_raw.csv\"))\n",
    "Y_val = pd.read_csv(os.path.join(folder,\"Y_val_raw.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a copy of *X_train* which will be transformed for application of K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans = X_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply logarithm transform (if needed) and standard scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_transform:\n",
    "    X_kmeans[X_kmeans < threshold] = threshold\n",
    "\n",
    "    # Apply log\n",
    "    X_kmeans.iloc[:, 1:] = np.log(X_kmeans.iloc[:, 1:])\n",
    "\n",
    "# Apply scaling\n",
    "Xscaler = StandardScaler()\n",
    "Xscaler.fit(X_kmeans)\n",
    "X_kmeans = Xscaler.transform(X_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the K-means clustering. The number of clusters has to be manually prescribed with this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then get the list of labels for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clusters_train = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the labels for the validation data, we apply the K-Means algorithm to *X_val*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data Kmeans\n",
    "X_kmeans_val = X_val.copy()\n",
    "# Apply threshold if log\n",
    "if log_transform:\n",
    "    X_kmeans_val[X_kmeans_val < threshold] = threshold\n",
    "\n",
    "    # Apply log\n",
    "    X_kmeans_val.iloc[:, 1:] = np.log(X_kmeans_val.iloc[:, 1:])\n",
    "\n",
    "# Apply scaling\n",
    "X_kmeans_val = Xscaler.transform(X_kmeans_val)\n",
    "\n",
    "kmeans_clusters_val = kmeans.predict(X_kmeans_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the K-means and the associated scaler in the folder for a later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_kmeans = os.path.join(folder, \"KMEANS\")\n",
    "if not os.path.isdir(folder_kmeans):\n",
    "    os.mkdir(folder_kmeans)\n",
    "\n",
    "# Saving K-means model\n",
    "with open(os.path.join(folder_kmeans, \"kmeans_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "# Saving scaler\n",
    "joblib.dump(Xscaler, os.path.join(folder_kmeans,\"Xscaler_kmeans.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the clusters on a scatter plot for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "scatter = ax.scatter(X_train[\"Temperature_X\"], X_train[\"H2_X\"], c=kmeans_clusters_train, cmap=cmap)\n",
    "\n",
    "ax.set_ylabel(r\"$Y_{H_2}$ $[-]$\", fontsize=14)\n",
    "ax.set_xlabel(r\"$T$ $[K]$\", fontsize=14)\n",
    "\n",
    "cbar = fig.colorbar(scatter, ax=ax, ticks=np.unique(kmeans_clusters_train))\n",
    "cbar.ax.tick_params(size=0, labelsize=14)\n",
    "\n",
    "fig.savefig(os.path.join(folder,\"KMEANS/clustering_plot.png\"), dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for training\n",
    "\n",
    "If we want to use the K-means clustered data for training ANN models, we need to define separate scalers for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_cluster_i(log_transform, threshold, i_cluster):\n",
    "\n",
    "    dtb_folder_i = os.path.join(folder, f\"KMEANS/cluster_{i_cluster}\")\n",
    "    if not os.path.isdir(dtb_folder_i):\n",
    "        os.mkdir(dtb_folder_i)\n",
    "\n",
    "    # Getting data for cluster\n",
    "    X_train_i = X_train[kmeans_clusters_train==i_cluster].copy()\n",
    "    Y_train_i = Y_train[kmeans_clusters_train==i_cluster].copy()\n",
    "    #\n",
    "    X_val_i = X_val[kmeans_clusters_val==i_cluster].copy()\n",
    "    Y_val_i = Y_val[kmeans_clusters_val==i_cluster].copy()\n",
    "\n",
    "    print(f\"CLUSTER {i_cluster}\")\n",
    "    print(f\" >> {X_train_i.shape[0]} points in training set\")\n",
    "    print(f\" >> {X_val_i.shape[0]} points in training set \\n\")\n",
    "\n",
    "    # Apply threshold if log\n",
    "    if log_transform:\n",
    "        X_train_i[X_train_i < threshold] = threshold\n",
    "        X_val_i[X_val_i < threshold] = threshold\n",
    "        #\n",
    "        Y_train_i[Y_train_i < threshold] = threshold\n",
    "        Y_val_i[Y_val_i < threshold] = threshold\n",
    "\n",
    "        # Apply log\n",
    "        X_train_i.iloc[:, 1:] = np.log(X_train_i.iloc[:, 1:])\n",
    "        X_val_i.iloc[:, 1:] = np.log(X_val_i.iloc[:, 1:])\n",
    "        #\n",
    "        Y_train_i = np.log(Y_train_i)\n",
    "        Y_val_i = np.log(Y_val_i)\n",
    "\n",
    "\n",
    "    # Apply scaling\n",
    "    Xscaler = StandardScaler()\n",
    "    Xscaler.fit(X_train_i)\n",
    "    X_train_i = Xscaler.transform(X_train_i)\n",
    "    X_val_i = Xscaler.transform(X_val_i)\n",
    "\n",
    "    Yscaler = StandardScaler()\n",
    "    Yscaler.fit(Y_train_i)\n",
    "    Y_train_i = Yscaler.transform(Y_train_i)\n",
    "    Y_val_i = Yscaler.transform(Y_val_i)\n",
    "\n",
    "    # Saving scalers for later use\n",
    "    joblib.dump(Xscaler, os.path.join(dtb_folder_i,'Xscaler.pkl'))\n",
    "    joblib.dump(Yscaler, os.path.join(dtb_folder_i,'Yscaler.pkl'))\n",
    "\n",
    "\n",
    "    # Saving data (transformed)\n",
    "    X_train_i.to_csv(os.path.join(dtb_folder_i,\"X_train.csv\"), index=False)\n",
    "    Y_train_i.to_csv(os.path.join(dtb_folder_i,\"Y_train.csv\"), index=False)\n",
    "    X_val_i.to_csv(os.path.join(dtb_folder_i,\"X_val.csv\"), index=False)\n",
    "    Y_val_i.to_csv(os.path.join(dtb_folder_i,\"Y_val.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_cluster in range(n_clusters):\n",
    "    preproc_cluster_i(log_transform, threshold, i_cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
