{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning applied to 0D reactors\n",
    "\n",
    "In this notebook, we will train artificial neural networks (ANNs) to replace the call to the CVODE solver of CANTERA. We will use the databases generated in the *0D_database_generation.ipynb* notebook (make sure that the database is generated and preprocessed properly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab preparation\n",
    "\n",
    "These lines are here to enable Colab running of the tools. We need to perform a git clone in order to have access to python scripts. This needs to be done at each runtime as the clone is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if use_colab:\n",
    "    !git clone -b main https://github.com/cmehl/COST_lecture.git\n",
    "    \n",
    "    !pip install cantera\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Create a folder in the root directory\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/COST_lecture_data\"):\n",
    "        !mkdir -p \"/content/drive/MyDrive/COST_lecture_data\"\n",
    "    else:\n",
    "        print(\"Folder /content/drive/MyDrive/COST_lecture_data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cantera as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(\"notebook\")\n",
    "\n",
    "if use_colab:\n",
    "    from COST_lecture.chem_ai.cantera_runs import compute_nn_cantera_0D_homo\n",
    "    from COST_lecture.chem_ai.utils import get_molar_mass_atomic_matrix\n",
    "    from COST_lecture.chem_ai.utils import StandardScaler\n",
    "else:\n",
    "    from chem_ai.cantera_runs import compute_nn_cantera_0D_homo\n",
    "    from chem_ai.utils import get_molar_mass_atomic_matrix\n",
    "    from chem_ai.utils import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the default Pytorch precision to double. It slows down a little bit the training but it is the usual standard for CFD reacting flows applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the device (CPU or GPU) available on the machine. This will be used by Pytorch to identify the device on which to train and use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### Loading the required data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the folder including the desired database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    folder = \"/content/drive/MyDrive/ML_chem_data/case_0D_test\"\n",
    "else:\n",
    "    folder = \"./case_0D_test\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the parameters stored in the json file of the dabatase folder: (please refer to the database generation notebook for the meaning of the parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder, \"dtb_params.json\"), \"r\") as file:\n",
    "    dtb_params = json.load(file)\n",
    "\n",
    "fuel = dtb_params[\"fuel\"]\n",
    "mech_file = dtb_params[\"mech_file\"]\n",
    "log_transform = dtb_params[\"log_transform\"]\n",
    "threshold = dtb_params[\"threshold\"]\n",
    "p = dtb_params[\"p\"]\n",
    "dt = dtb_params[\"dt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the scalers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Xscaler.pkl\"))\n",
    "Yscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Yscaler.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training and validation databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join(folder, \"processed_database\",\"X_train.csv\"))\n",
    "X_val = pd.read_csv(os.path.join(folder, \"processed_database\",\"X_val.csv\"))\n",
    "Y_train = pd.read_csv(os.path.join(folder, \"processed_database\",\"Y_train.csv\"))\n",
    "Y_val = pd.read_csv(os.path.join(folder, \"processed_database\",\"Y_val.csv\"))\n",
    "\n",
    "Xcols = X_train.columns\n",
    "Ycols = Y_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of input and output dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = X_train.shape[1]\n",
    "n_out = Y_train.shape[1]\n",
    "\n",
    "print(f\"Number of inputs: {n_in}\")\n",
    "print(f\"Number of outputs: {n_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements conservation matrix\n",
    "\n",
    "In combustion, elements (usually C, H, O, N) are preserved when a mixture undergoes chemical reactions, as there are no nuclear reactions. Therefore, the initial mass of elements of a mixture is conserved at the next time step and so on. For a 0D reactors (no mixing), elements mass fractions are constant for a given simulation. We briefly recall here the material presented in the lecture slides.\n",
    "\n",
    "For a given element $j \\in {C, H, O, N}$, the mass fraction of this elements can be expressed as:\n",
    "\n",
    "$$\n",
    "Y_e^j = \\sum_{k=1}^{N_S} \\frac{M_j}{M_k} n_k^j Y_k\n",
    "$$\n",
    "\n",
    "where $M_j$ and $M_k$ are the molar masses of element $j$ and species $k$ respectively. $n_k^j$ is the number of atoms $j$ in species $k$. This equation can also be written in matrix form:\n",
    "\n",
    "$$\n",
    "Y_e = \\mathcal{A} Y\n",
    "$$\n",
    "\n",
    "where $Y_e \\in \\mathbb{R}^4$ is the vector of elements mass fractions and $Y \\in \\mathbb{R}^{N_S}$ the vector of species mass fractions. The matrix $\\mathcal{A} \\in \\mathbb{R}^{4 \\times N_S}$ is defined by the following coefficients:\n",
    "\n",
    "$$\n",
    "\\mathcal{A}_{jk} = \\frac{M_j}{M_k} n_k^j\n",
    "$$\n",
    "\n",
    "The matrix $\\mathcal{A}$ can be computed using the function *get_molar_mass_atomic_matrix* that is provided in *utils.py*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = ct.Solution(mech_file)\n",
    "A_element = get_molar_mass_atomic_matrix(gas.species_names, fuel, True)\n",
    "print(A_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix will be helpful to analyze the conservation of elements in the training loop and at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loaded (scalers, training/validation sets, etc...) are in numpy format. In order to use them in a Pytorch training loop, we need to convert them to *torch* tensors. Those tensors are very similar to numpy arrays, with similar functions.\n",
    "\n",
    "We first transform training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "Y_train = torch.tensor(Y_train.values, dtype=torch.float64)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float64)\n",
    "Y_val = torch.tensor(Y_val.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with the scaler, we decide to extract the mean and standard deviation and write the formula explicitly when necessary (making the *StandardScaler* class Pytorch-compatible is cumbersome). These quantities are here converted to torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler_mean = torch.from_numpy(Xscaler.mean.values)\n",
    "Xscaler_std = torch.from_numpy(Xscaler.std.values)\n",
    "\n",
    "Yscaler_mean = torch.from_numpy(Yscaler.mean.values)\n",
    "Yscaler_std = torch.from_numpy(Yscaler.std.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conservation matrix $A$ also needs to be converted as it will be used during the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_element = torch.tensor(A_element, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect is that the data needs to be on the correct device, as pytorch will look for the data on it. As CPU and GPU memory is not shared, we will have to manually move the data to the GPU if necessary. We do it here for training/validation data, scalers and conservation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "\n",
    "Xscaler_mean = Xscaler_mean.to(device)\n",
    "Xscaler_std = Xscaler_std.to(device)\n",
    "\n",
    "Yscaler_mean = Yscaler_mean.to(device)\n",
    "Yscaler_std = Yscaler_std.to(device)\n",
    "\n",
    "A_element = A_element.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can generate the model. In this work, we will consider a simple Multi Layer Perceptron (MLP). We generate the model using Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the number of layers and units in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then instantiated and transferred to the GPU if present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChemNN()\n",
    "print(model)\n",
    "\n",
    "#Put model on GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define hyperparameters of the training loop. The following choices need to be made:\n",
    "\n",
    "+ **n_epochs**: number of passes of entire training dataset through the algorithm.\n",
    "+ **batch_size**: size of the chunks passed to the algorithm at each parameters update.\n",
    "+ **loss_fn**: loss function. In this work we choose the Mean Square Loss (MSE), which is adapted to the regression problem and classical used in this context. Assuming that the output of the ANN is $Y_k^n$ (preprocessed mass fractions) and the true value is $Y_k^{n,*}$, the loss reads:\n",
    "\n",
    "$$ \n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{N_S} \\sum_{k=1}^{N_S} \\left( Y_{k,i}^n - Y_{k,i}^{n,*} \\right)^2\n",
    "$$\n",
    "\n",
    "where $N$ is the number of data points. Other loss functions (such as MAE) are also available in Pytorch, feel free to test them (go to the [Pytorch website](https://pytorch.org/docs/stable/nn.html) for details)\n",
    "\n",
    "+ **optimizer**: optimization method. We use here the standard Adam method with initial learning rate $lr$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "batch_size = 256\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform now the main model training loop. In Pytorch the training loop needs to be written but offers flexibility in the way we can compute training monitoring quantities. In this loop, we decide to monitor conservation metrics, i.e. the sum of species mass fractions and the elements mass fractions variation.\n",
    "\n",
    "*Exercice 1:* Complete the training loop by computing monitoring metrics. Note that these metrics are computed every 10 epochs in order to limit the computational overhead.\n",
    "\n",
    "1. From the conservation of mass we have $\\sum_{k=1}^{N_S} Y_k = 1$. Compute the mean, min and max of $\\sum_{k=1}^{N_S} Y_k$ over the entire validation dataset. \n",
    "\n",
    "2. Let us note $Y^{in}$ the mass fraction of species at the input of the ANN and $Y^{out}$ at the output (same for elemental mass fractions which are written $Y_e^{in}$ and $Y_e^{out}$). Elements conservation imposes $Y_e^{in}=Y_e^{out}$ which means $\\mathcal{A}Y^{in}=\\mathcal{A}Y^{out}$. We will consider here the quantity $\\delta Y_e = \\left( \\mathcal{A}Y^{out} - \\mathcal{A}Y^{in} \\right) / \\mathcal{A}Y^{in} \\in \\mathbb{R}^4$. Compute the mean, min and max of this quantity over the entire validation dataset.\n",
    "\n",
    "We give the following hints:\n",
    "\n",
    "+ Do not forget that input of output data of the ANN is preprocessed. For the scaling, you can use directly the mean and std values and the formula, as these vectors have been put to the GPU to that purpose.\n",
    "\n",
    "+ Metric arrays are defined as numpy arrays on CPU to be later plotted on matplotlib. In order to transfer a GPU torch tensor *tensor_torch* to a CPU numpy array you can use: *tensor_torch.detach().cpu().numpy()*.\n",
    "\n",
    "+ Matrix multiplication in pytorch can be done using the *torch.matmul* function and transpose by using *torch.transpose*.\n",
    "\n",
    "+ Think about the dimensions of the tensors you are trying to multiply. Sometimes the transpose can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform, need_ini_vals):\n",
    "\n",
    "    # Array to store the loss and validation loss\n",
    "    loss_list = np.empty(n_epochs)\n",
    "    val_loss_list = np.empty(n_epochs//10)\n",
    "\n",
    "    # Array to store sum of mass fractions: mean, min and max\n",
    "    stats_sum_yk = np.empty((n_epochs//10,3))\n",
    "\n",
    "    # Array to store elements conservation: mean, min and max\n",
    "    stats_A_elements = np.empty((n_epochs//10,4,3))\n",
    "\n",
    "    epochs = np.arange(n_epochs)\n",
    "    epochs_small = epochs[::10]\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training parameters\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "\n",
    "            Xbatch = X_train[i:i+batch_size]\n",
    "            y_pred = model(Xbatch)\n",
    "            ybatch = Y_train[i:i+batch_size]\n",
    "            if need_ini_vals: # used for soft elements constraint later\n",
    "                loss = loss_fn(y_pred, ybatch, Xbatch[:,1:])\n",
    "            else:\n",
    "                loss = loss_fn(y_pred, ybatch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_list[epoch] = loss\n",
    "\n",
    "        # Computing validation loss and mass conservation metric (only every 10 epochs as it is expensive)\n",
    "        if epoch%10==0:\n",
    "            model.eval()  # evaluation mode\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # VALIDATION LOSS\n",
    "                y_val_pred = model(X_val)\n",
    "                if need_ini_vals:\n",
    "                    val_loss = loss_fn(y_val_pred, Y_val, X_val[:,1:])\n",
    "                else:\n",
    "                    val_loss = loss_fn(y_val_pred, Y_val)\n",
    "\n",
    "                ### TO COMPLETE ###\n",
    "                # SUM OF MASS FRACTION\n",
    "                #Inverse scale done by hand to stay with Torch arrays\n",
    "                yk = ...\n",
    "                # Inverting the log\n",
    "                ...\n",
    "                # Computing sum of Yk\n",
    "                sum_yk = ...\n",
    "\n",
    "                # Getting the statistics\n",
    "                stats_sum_yk[epoch//10,0] = ...\n",
    "                stats_sum_yk[epoch//10,1] = ...\n",
    "                stats_sum_yk[epoch//10,2] = ...\n",
    "                ### END ###\n",
    "\n",
    "                ### TO COMPLETE ###\n",
    "                # ELEMENTS CONSERVATION\n",
    "                # We need to get non transformed ANN input layer\n",
    "                yval_in = ...\n",
    "                # Inverting the log\n",
    "                ...\n",
    "                # elements vectors\n",
    "                ye_in = ...\n",
    "                ye_out = ...\n",
    "                delta_ye = ... \n",
    "\n",
    "                # Statistics\n",
    "                stats_A_elements[epoch//10, :, 0] = ...\n",
    "                stats_A_elements[epoch//10, :, 1] = ...\n",
    "                stats_A_elements[epoch//10, :, 2] = ...\n",
    "                ### END ###\n",
    "\n",
    "            model.train()   # Back to training mode\n",
    "            val_loss_list[epoch//10] = val_loss\n",
    "\n",
    "        print(f\"Finished epoch {epoch}\")\n",
    "        print(f\"    >> Loss: {loss}\")\n",
    "        if epoch%10==0:\n",
    "            print(f\"    >> Validation loss: {val_loss}\")\n",
    "\n",
    "    return epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform, False)\n",
    "end_time = time.perf_counter()\n",
    "print(f\" TRAINING DURATION: {end_time-start_time} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function to analyze the training. We plot:\n",
    "\n",
    "+ The training and validation losses\n",
    "\n",
    "+ The evolution of $\\sum_{k=1}^{N_S} Y_k$ (mean, min and max).\n",
    "\n",
    "+ The elements conservation by plotting $100\\times\\delta Y_e$ for each element (C, H, O and N). The factor $100$ enables to get an error in \\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements):\n",
    "\n",
    "    # LOSSES\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs, loss_list, color=\"k\", label=\"Training\")\n",
    "    ax.plot(epochs_small, val_loss_list, color=\"r\", label = \"Validation\")\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    # MASS CONSERVATION\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,0], color=\"k\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,1], color=\"k\", ls=\"--\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(r\"$\\sum_k \\ Y_k$\")\n",
    "\n",
    "    # ELEMENTS CONSERVATION\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "\n",
    "    # C\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,0], color=\"k\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,1], color=\"k\", ls=\"--\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(r\"$\\Delta Y_C$ $(\\%$)\")\n",
    "\n",
    "    # H\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,0], color=\"k\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,1], color=\"k\", ls=\"--\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(r\"$\\Delta Y_H$ $(\\%)$\")\n",
    "\n",
    "    # O\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,0], color=\"k\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,1], color=\"k\", ls=\"--\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(r\"$\\Delta Y_O$ $(\\%)$\")\n",
    "\n",
    "    # N\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,0], color=\"k\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,1], color=\"k\", ls=\"--\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(r\"$\\Delta Y_N$ $(\\%)$\")\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the Pytorch model in the case folder for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN model test\n",
    "\n",
    "Now that we have generated the model we would like to test it on unseen data. For this, we will use the test initial conditions which were stored during data generation. The methodology is as follows:\n",
    "\n",
    "1. We get the test conditions and simulate CANTERA flames with (i) the CVODE solver and (ii) the generated ANN.\n",
    "2. We define appropriate metric and assess the accuracy of the ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing simulations with CANTERA and NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the test initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_test = pd.read_csv(os.path.join(folder, \"sim_test.csv\"))\n",
    "\n",
    "n_sim = df_sim_test.shape[0]\n",
    "print(f\"There are {n_sim} test simulations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compute 0D reactors for each conditions in *df_sim_test*. We use here the function *compute_nn_cantera_0D_homo* which is provided. It takes as input initial conditions $T_0, \\phi$, the ANN model (with associated scalers), the time step (used for the ANN) and the database parameters. It outputs two dataframes containing (i) the exact simulation and (ii) the ANN simulation. These results are concatenated in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results = []\n",
    "\n",
    "fails = 0\n",
    "for i, row in df_sim_test.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn, fail = compute_nn_cantera_0D_homo(device, model, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params, A_element.detach().cpu().numpy())\n",
    "\n",
    "    fails += fail\n",
    "\n",
    "    list_test_results.append((df_exact, df_nn))\n",
    "\n",
    "\n",
    "print(f\"Total number of simulations which crashed: {fails}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are what dataframes look like for a given simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 1\n",
    "list_test_results[i_sim][0].head()   # Exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results[i_sim][1].head()   # ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the results for a given simulation *i_sim*. We can look at temperature and species mass fractions for instance. We can also analyze conservation metrics. If we learned in logarithmic space, it is interesting also to plot in log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_sim(i_sim, list_test_results, spec_to_plot):\n",
    "\n",
    "    df_exact = list_test_results[i_sim][0]\n",
    "    df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "    # Temperature \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact['Temperature'], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn['Temperature'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"T [K]\")\n",
    "\n",
    "    # Species (normal)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact[spec_to_plot], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn[spec_to_plot], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Species (log)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], np.log(df_exact[spec_to_plot]), color='k')\n",
    "    ax.plot(df_nn['Time'], np.log(df_nn[spec_to_plot]), color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Sum of Yk\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_nn['Time'], df_nn['SumYk'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"$\\sum Y_k$ [-]\")\n",
    "\n",
    "    # Elements\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "    ax1.plot(df_nn['Time'], df_nn['Y_C'], color='b')\n",
    "    ax2.plot(df_nn['Time'], df_nn['Y_H'], color='b')\n",
    "    ax3.plot(df_nn['Time'], df_nn['Y_O'], color='b')\n",
    "    ax4.plot(df_nn['Time'], df_nn['Y_N'], color='b')\n",
    "    ax1.set_ylabel(\"$Y_C$\")\n",
    "    ax2.set_ylabel(\"$Y_H$\")\n",
    "    ax3.set_ylabel(\"$Y_O$\")\n",
    "    ax4.set_ylabel(\"$Y_N$\")\n",
    "    ax3.set_xlabel(\"Time [s]\")\n",
    "    ax4.set_xlabel(\"Time [s]\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 40\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim(i_sim, list_test_results, spec_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing error statistics\n",
    "\n",
    "We need a metric to assess the accuracy of the ANN over the entire test simulations. To do that, we will define normalized fitness functions for each simulation, and average the values over the simulation. As previously, we note $Y_k^n$ the processed mass fractions (including potential log and scaling) from the ANN, and $Y_k^{n,*}$ the exact values. The error on species mass fractions is then for given initial conditions:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_k(T_0,\\phi) = \\frac{1}{N_{iter}} \\sum_{i=1}^{N_{iter}} \\left| \\frac{Y_k^n - Y_k^{n,*}}{Y_k^{n,*}} \\right|\n",
    "$$\n",
    "\n",
    "where $N_{iter}$ is the number of iterations of the considered simulation (it may vary from one simulation to another as it is controlled by a stopping criterion).\n",
    "\n",
    "Although it is not a direct output of the model, we can still compute error on the temperature. We write $T^n$ and $T^{n,*}$ the normalized predicted and exact temperatures, respestively (note that log is never applied on temeprature). The error is:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_T(T_0,\\phi) = \\frac{1}{N_{iter}} \\sum_{i=1}^{N_{iter}} \\left| \\frac{T^n - T^{n,*}}{T^{n,*}} \\right|\n",
    "$$\n",
    "\n",
    "Finally, we can define a global error for each simulation as the mean of all errors (they can be compared as everything is normalized):\n",
    "\n",
    "$$\n",
    "\\mathcal{M}(T_0,\\phi) = \\frac{\\mathcal{M}_T(T_0,\\phi) + \\sum_{k=1}^{N_S} \\mathcal{M}_k(T_0,\\phi)}{N_S+1}\n",
    "$$\n",
    "\n",
    "*Exercice 2:* we define a function *compute_fitness* to calculate these errors. Complete the calculation of the errors in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fitness(list_test_results):\n",
    "\n",
    "    # Results will be stored in data_errors array.\n",
    "    # The first column corresponds to errors on temperature\n",
    "    # The next n_out columns correspond to errors on species mass fractions\n",
    "    # The last column corresponds to the mean error\n",
    "    data_errors = np.empty([n_sim, n_out+2]) \n",
    "\n",
    "    for i_sim in range(n_sim):\n",
    "\n",
    "        df_exact = list_test_results[i_sim][0]\n",
    "        df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "        # Removing undesired variables\n",
    "        df_exact = df_exact.drop('Time', axis=1)\n",
    "        df_nn = df_nn.drop([\"Time\",\"SumYk\", \"Y_C\", \"Y_H\", \"Y_O\", \"Y_N\"], axis=1)\n",
    "\n",
    "        # Applying log\n",
    "        if log_transform:\n",
    "            ### TO COMPLETE ###\n",
    "            # Apply threshold\n",
    "            ...\n",
    "\n",
    "            df_exact.iloc[:, 1:] = ...\n",
    "            df_nn.iloc[:, 1:] = ...\n",
    "            ### END ###\n",
    "\n",
    "        ### TO COMPLETE ###\n",
    "        # Scaling\n",
    "        data_exact_scaled = ...\n",
    "        data_nn_scaled = ...\n",
    "\n",
    "        # Pointwise error\n",
    "        diff_exact_nn = ...\n",
    "\n",
    "        # Mean over iterations\n",
    "        diff_exact_nn = ...\n",
    "\n",
    "        # Mean over species\n",
    "        M = ...\n",
    "        ### END ###\n",
    "\n",
    "        print(f\"Simulation {i_sim} error M = {M}\")\n",
    "\n",
    "        data_errors[i_sim, :n_out+1] = diff_exact_nn\n",
    "        data_errors[i_sim, n_out+1] = M\n",
    "\n",
    "\n",
    "    return data_errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors = compute_fitness(list_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize errors, we can draw a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting gas species for labels\n",
    "gas = ct.Solution(mech_file)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(data_errors, ax=ax)\n",
    "\n",
    "custom_labels = [\"T\"] + gas.species_names + [\"Total\"]\n",
    "ax.set_xticklabels(custom_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 3*: Find the simulation with the highest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO COMPLETE ###\n",
    "i_sim_max = ...\n",
    "### END ###\n",
    "print(f\"Simulation with largest error: {i_sim_max}\")\n",
    "print(f\"Error is: {data_errors[:,-1][i_sim_max]} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can average the errors of all simulations to get a global error which can be used to get an idea of the overall error of the ANN:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_{avg} = \\frac{1}{N_{test}} \\sum_{i=1}^{N_{test}} \\mathcal{M}(T_{0,i},\\phi_i)\n",
    "$$\n",
    "\n",
    "where $N_{test}$ is the number of test simulations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vect = data_errors[:,-1]\n",
    "\n",
    "print(f\"Averaged on set of test simulations, error is M={M_vect.mean()} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results must be taken with caution, as the fitness function is not perfect. Amongst potential issues we can note the fact that when profiles are close to $0$, it can lead to large errors (for example if we predict $10^{-6}$ instead of $10^{-7}$ for a major species). This will artificially increase errors.\n",
    "\n",
    "Other fitness functions are possible but not explored here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforcing physical information: soft constraints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the strategy employed above, the ANN model freely predicts the new chemical states without being forced to satisfy any constraints. As already analyzed above and explained in the lecture slides, several constraints must be satisfied. First, mass fractions should sum up to $1$:\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{N_S} Y_k = 1\n",
    "$$\n",
    "\n",
    "Additionally, elements must be conserved. This amounts to say that the elements mass fractions must be equal for the ANN inputs and outputs:\n",
    "\n",
    "$$\n",
    "Y_e^{in} = Y_e^{out}\n",
    "$$\n",
    "\n",
    "One way to lead the ANN towards verifying constraints is to add penalization terms in the loss functions. Those are then called **soft constraints**. In the present case, we will consider two loss functions. The first one tries to impose mass conservation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{mass} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{N_S} \\sum_{k=1}^{N_S} \\left( Y_{k,i}^n - Y_{k,i}^{n,*} \\right)^2 + \\alpha_{mass} \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{k=1}^{N_S} Y_k - 1 \\right)^2\n",
    "$$\n",
    "\n",
    "where $\\alpha_{mass}$ is a hyperparameter to control the constraint weight in the loss.\n",
    "\n",
    "Another loss can be defined by adding a penalty term for elements conservation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{elt} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{N_S} \\sum_{k=1}^{N_S} \\left( Y_{k,i}^n - Y_{k,i}^{n,*} \\right)^2 + \\alpha_{elt} \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{4} \\sum_{j=1}^{4} \\left( Y_{e,j}^{out} - Y_{e,j}^{in} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 4:* complete the loss function for mass conservation.\n",
    "\n",
    "*Hint*: use *torch.mean* in order to get the average of a tensor in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sumYkLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(sumYkLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, targets):\n",
    "\n",
    "        ### TO COMPLETE ###\n",
    "        # Inverse transform\n",
    "        yk = ...\n",
    "        if log_transform:\n",
    "            ...\n",
    "        # Computing sum of Yks\n",
    "        sum_yk = ...\n",
    "\n",
    "        # Computing loss\n",
    "        loss = ...\n",
    "        ### END ### \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 5:* complete the loss function for elements conservation.\n",
    "\n",
    "*Hint*: do not forget to think about the dimensions of the tensors when you multiply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElementLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(ElementLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, targets, yk_scaled_in):\n",
    "\n",
    "        ### TO COMPLETE ###\n",
    "        # Elements input\n",
    "        yk_in = ...\n",
    "        if log_transform:\n",
    "            ...\n",
    "        ye_in = ...\n",
    "\n",
    "        # Elements output\n",
    "        yk = ...\n",
    "        if log_transform:\n",
    "            yk = ...\n",
    "        ye_out = ...\n",
    "\n",
    "        loss = ...\n",
    "        ### END ###\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin from the model already trained without constraints. You can also choose to train a new model from scratch. We can redo the training loop and setting one of the new losses: (feel free to test both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "alpha = 100.0\n",
    "\n",
    "# loss_fn = sumYkLoss(alpha)\n",
    "# need_ini_val = False\n",
    "\n",
    "loss_fn = ElementLoss(alpha)\n",
    "need_ini_val = True\n",
    "\n",
    "epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform, need_ini_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp_soft.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot again the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recompute the test simulations using the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results = []\n",
    "\n",
    "fails = 0\n",
    "for i, row in df_sim_test.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn, fail = compute_nn_cantera_0D_homo(device, model, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params, A_element.detach().cpu().numpy())\n",
    "\n",
    "    fails += fail\n",
    "\n",
    "    list_test_results.append((df_exact, df_nn))\n",
    "\n",
    "\n",
    "print(f\"Total number of simulations which crashed: {fails}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again plot results for a given simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 40\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim(i_sim, list_test_results, spec_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors_soft_cst = compute_fitness(list_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting of the errors distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting gas species for labels\n",
    "gas = ct.Solution(mech_file)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(data_errors_soft_cst, ax=ax)\n",
    "\n",
    "custom_labels = [\"T\"] + gas.species_names + [\"Total\"]\n",
    "ax.set_xticklabels(custom_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vect = data_errors_soft_cst[:,-1]\n",
    "\n",
    "print(f\"Averaged on set of test simulations, error is M={M_vect.mean()} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim_max = data_errors_soft_cst[:,-1].argmax()\n",
    "print(f\"Simulation with largest error: {i_sim_max}\")\n",
    "print(f\"Error is: {data_errors_soft_cst[:,-1][i_sim_max]} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the *0D_database_generation.ipynb* and the present notebook, you can play around and do some tests on your own. Here are some ideas:\n",
    "\n",
    "+ Variation of the sampling space: extension of the $T_0$ and/or $\\phi$ range, change of the pressure.\n",
    "\n",
    "+ Change of the number of $(\\phi, T_0)$ samples.\n",
    "\n",
    "+ Applying methodology with another (bigger) chemical mechanism. The mechanism *mech_ch4_lu30.yaml* is provided, it is a 30 species mechanism for methane ($CH_4$) combustion.\n",
    "\n",
    "+ Or any other idea you might have.\n",
    "\n",
    "You will see that depending on the problem, making things run might need a few trial and errors step and playing around with hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
